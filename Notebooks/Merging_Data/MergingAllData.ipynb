{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "19e22c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import WranglerFunctions as wf\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b348d7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeToOne(df1, df2):\n",
    "    merge = (df1.merge(df2,how='left', left_on= 'Date', right_on = 'Date'))\n",
    "    return merge\n",
    "    \n",
    "def RenameDateTimeColumnName(df):\n",
    "    df_dates = df.select_dtypes(include='datetime')\n",
    "    datetime_columns = df_dates.columns\n",
    "    \n",
    "    if len(datetime_columns) == 0:\n",
    "        df.reset_index(inplace=True)\n",
    "        df_dates = df.select_dtypes(include='datetime')\n",
    "        datetime_columns = df_dates.columns\n",
    "        \n",
    "    df_date_column_name = datetime_columns[0]\n",
    "        \n",
    "    if df_date_column_name != 'Date':\n",
    "        df.rename(columns = {df_date_column_name : 'Date'}, inplace = True)\n",
    "        \n",
    "def fillInNanValues(df):\n",
    "    #return df.bfill().ffill()\n",
    "    return df.ffill().bfill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6787919c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDailyDataframe(start_date, end_date):\n",
    "    df_dates = pd.date_range(start = start_date, end = end_date)\n",
    "    df = df_dates.to_frame(name = 'Date')\n",
    "    df.index = range(len(df))\n",
    "    return df\n",
    "\n",
    "def addNewDataframe(total_df, new_df, new_df_name):\n",
    "    if new_df_name != \"\":\n",
    "        new_df = new_df.add_prefix(new_df_name + \"_\")                        #adds df name as prefix to all its columns\n",
    "    RenameDateTimeColumnName(new_df) #finds and renames date column to the same as the whole table\n",
    "    total_df = mergeToOne(total_df, new_df)\n",
    "    return total_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cdbf0bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_start_date = '01/01/1990'\n",
    "initial_end_date   = '12/04/2022'\n",
    "whole_df = createDailyDataframe(initial_start_date, initial_end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772ee9e0",
   "metadata": {},
   "source": [
    "#### Adding Commodities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a1b51e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Dates  PX_LAST\n",
      "0    04/05/2022   104.33\n",
      "1    03/31/2022   100.28\n",
      "2    02/28/2022    95.72\n",
      "3    01/31/2022    88.15\n",
      "4    12/31/2021    75.21\n",
      "..          ...      ...\n",
      "296  08/29/1997    19.61\n",
      "297  07/31/1997    20.14\n",
      "298  06/30/1997    19.80\n",
      "299  05/30/1997    20.88\n",
      "300  04/30/1997    20.21\n",
      "\n",
      "[301 rows x 2 columns]\n",
      "          Dates  PX_LAST\n",
      "0    04/04/2022   3447.0\n",
      "1    03/31/2022   3491.0\n",
      "2    02/28/2022   3368.5\n",
      "3    01/31/2022   3021.0\n",
      "4    12/31/2021   2807.5\n",
      "..          ...      ...\n",
      "296  08/29/1997   1631.5\n",
      "297  07/31/1997   1700.0\n",
      "298  06/30/1997   1598.0\n",
      "299  05/30/1997   1619.0\n",
      "300  04/30/1997   1645.0\n",
      "\n",
      "[301 rows x 2 columns]\n",
      "          Dates  PX_LAST\n",
      "0    04/04/2022   2413.0\n",
      "1    03/31/2022   2416.0\n",
      "2    02/28/2022   2387.0\n",
      "3    01/31/2022   2244.5\n",
      "4    12/31/2021   2304.0\n",
      "..          ...      ...\n",
      "296  08/29/1997    652.0\n",
      "297  07/31/1997    632.0\n",
      "298  06/30/1997    627.0\n",
      "299  05/30/1997    633.0\n",
      "300  04/30/1997    624.0\n",
      "\n",
      "[301 rows x 2 columns]\n",
      "          Dates  PX_LAST\n",
      "0    04/04/2022  44155.0\n",
      "1    03/31/2022  42910.0\n",
      "2    02/28/2022  45224.0\n",
      "3    01/31/2022  43023.0\n",
      "4    12/31/2021  38860.0\n",
      "..          ...      ...\n",
      "296  08/29/1997   5435.0\n",
      "297  07/31/1997   5545.0\n",
      "298  06/30/1997   5570.0\n",
      "299  05/30/1997   5565.0\n",
      "300  04/30/1997   5700.0\n",
      "\n",
      "[301 rows x 2 columns]\n",
      "          Dates     PX_LAST\n",
      "0    03/30/2022  7887.62988\n",
      "1    02/28/2022  7380.70361\n",
      "2    01/31/2022  7074.49561\n",
      "3    12/31/2021  7181.75098\n",
      "4    11/30/2021  7118.89307\n",
      "..          ...         ...\n",
      "275  04/30/1999   988.36847\n",
      "276  03/31/1999   864.04462\n",
      "277  02/26/1999   872.30328\n",
      "278  01/29/1999   870.12042\n",
      "279  12/31/1998   899.06598\n",
      "\n",
      "[280 rows x 2 columns]\n",
      "          Dates      PX_LAST\n",
      "0    03/30/2022  25025.10742\n",
      "1    02/28/2022  18133.07422\n",
      "2    01/31/2022  16616.80469\n",
      "3    12/31/2021  15335.79590\n",
      "4    11/30/2021  15000.75391\n",
      "..          ...          ...\n",
      "274  05/31/1999   2916.56250\n",
      "275  04/30/1999   3439.69653\n",
      "276  03/31/1999   2991.63818\n",
      "277  02/26/1999   3064.59668\n",
      "278  01/29/1999   2602.45654\n",
      "\n",
      "[279 rows x 2 columns]\n",
      "          Dates  PX_LAST\n",
      "0    04/04/2022  81822.0\n",
      "1    03/31/2022  81841.0\n",
      "2    02/28/2022  73718.0\n",
      "3    01/31/2022  70714.0\n",
      "4    12/31/2021  70193.0\n",
      "..          ...      ...\n",
      "139  09/30/2010  38800.0\n",
      "140  08/31/2010  41700.0\n",
      "141  07/30/2010  38100.0\n",
      "142  06/30/2010  38525.0\n",
      "143  05/31/2010  39125.0\n",
      "\n",
      "[144 rows x 2 columns]\n",
      "          Dates  PX_LAST\n",
      "0    03/31/2022  1045.94\n",
      "1    02/28/2022   869.18\n",
      "2    01/31/2022   624.49\n",
      "3    12/31/2021   461.73\n",
      "4    11/30/2021   412.47\n",
      "..          ...      ...\n",
      "154  05/31/2009   101.24\n",
      "155  04/30/2009   102.72\n",
      "156  03/31/2009   101.28\n",
      "157  02/28/2009   100.61\n",
      "158  01/31/2009   100.00\n",
      "\n",
      "[159 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vojno\\OneDrive - University of Bristol\\Applied Data Science\\Mining\\Notebooks\\Merging_Data\\WranglerFunctions.py:113: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  Freight_wrangle1 =df.drop(columns=[\"Unnamed: 1\",\"Unnamed: 2\",\"Unnamed: 3\",\"Unnamed: 4\"])\n",
      "C:\\Users\\vojno\\OneDrive - University of Bristol\\Applied Data Science\\Mining\\Notebooks\\Merging_Data\\WranglerFunctions.py:113: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  Freight_wrangle1 =df.drop(columns=[\"Unnamed: 1\",\"Unnamed: 2\",\"Unnamed: 3\",\"Unnamed: 4\"])\n"
     ]
    }
   ],
   "source": [
    "CL1_COMB_Comodity  =  wf.format_commodity_data_of_form_DATES_AND_PX_LAST('CL1_COMB_Comodity')     #Crude Oil\n",
    "whole_df            = addNewDataframe(whole_df, CL1_COMB_Comodity, \"CL1_COMB_Comodity\")\n",
    "\n",
    "LMAHDS03_LME_Comdty  =  wf.format_commodity_data_of_form_DATES_AND_PX_LAST('LMAHDS03_LME_Comdty') #Aluminium\n",
    "whole_df            = addNewDataframe(whole_df, LMAHDS03_LME_Comdty, \"LMAHDS03_LME_Comdty\")\n",
    "\n",
    "LMPBDS03_LME_Comdty  =  wf.format_commodity_data_of_form_DATES_AND_PX_LAST('LMPBDS03_LME_Comdty') #Lead\n",
    "whole_df            = addNewDataframe(whole_df, LMPBDS03_LME_Comdty, \"LMPBDS03_LME_Comdty\")\n",
    "\n",
    "LMSNDS03_LME_Comdty  =  wf.format_commodity_data_of_form_DATES_AND_PX_LAST('LMSNDS03_LME_Comdty') #Tin\n",
    "whole_df            = addNewDataframe(whole_df, LMSNDS03_LME_Comdty, \"LMSNDS03_LME_Comdty\")\n",
    "\n",
    "LMCADS03_LME_Comdty  =  wf.format_commodity_data_of_form_DATES_AND_PX_LAST('LMCADS03_LME_Comdty') #Copper\n",
    "whole_df            = addNewDataframe(whole_df, LMCADS03_LME_Comdty, \"LMCADS03_LME_Comdty\")\n",
    "\n",
    "LMNIDS03_LME_Comdty  =  wf.format_commodity_data_of_form_DATES_AND_PX_LAST('LMNIDS03_LME_Comdty') #Nickel\n",
    "whole_df            = addNewDataframe(whole_df, LMNIDS03_LME_Comdty, \"LMNIDS03_LME_Comdty\")\n",
    "\n",
    "LMCODY_LME_Comdty  =  wf.format_commodity_data_of_form_DATES_AND_PX_LAST('LMCODY_LME_Comdty')     #Cobalt -> Only goes to 05/31/2010\n",
    "whole_df            = addNewDataframe(whole_df, LMCODY_LME_Comdty, \"LMCODY_LME_Comdty\")\n",
    "\n",
    "LTBMPRIN_Index  =  wf.format_commodity_data_of_form_DATES_AND_PX_LAST('LTBMPRIN_Index')           #Lithium -> Only goes to 2009 \n",
    "whole_df            = addNewDataframe(whole_df, LTBMPRIN_Index, \"LTBMPRIN_Index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da4f489",
   "metadata": {},
   "source": [
    "#### Adding Inflation and Unemployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "be0ebce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_inflation = pd.read_csv('../../Notebooks/Datasets/Economic_dataset/Inflation.csv')\n",
    "inflation     = wf.Unemployment_Wrangler(raw_inflation)\n",
    "whole_df      = addNewDataframe(whole_df, inflation, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "97872e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_unemployment = pd.read_csv('../../Notebooks/Datasets/Economic_dataset/Unemployment.csv')\n",
    "unemployment     = wf.Unemployment_Wrangler(raw_unemployment)\n",
    "whole_df         = addNewDataframe(whole_df, unemployment, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2c1e29",
   "metadata": {},
   "source": [
    "#### Adding SPX500, GUKG10, F3METL, BCOMIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eac2c8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vojno\\AppData\\Local\\Temp\\ipykernel_21412\\1591766129.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.rename(columns = {df_date_column_name : 'Date'}, inplace = True)\n",
      "C:\\Users\\vojno\\AppData\\Local\\Temp\\ipykernel_21412\\1591766129.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.rename(columns = {df_date_column_name : 'Date'}, inplace = True)\n"
     ]
    }
   ],
   "source": [
    "def renameMultiIndex(df):\n",
    "    multiIndex = df.columns\n",
    "    newIndex   = []\n",
    "    for e in multiIndex:\n",
    "        string = e[0] + \" \" + e[1]\n",
    "        newIndex.append(string)\n",
    "    df.columns = newIndex\n",
    "    \n",
    "\n",
    "#S&P500 index\n",
    "original_SPX500 = pd.read_csv('../../Notebooks/Datasets/Financial_dataset/SPX500.csv')\n",
    "SPX500 = wf.SPX500_Wrangler(original_SPX500)\n",
    "SPX500_important = SPX500[[('SPX500 Index',   'PX_LAST'), ('SPX500 Index', 'PX_VOLUME')]]\n",
    "renameMultiIndex(SPX500_important)\n",
    "whole_df         = addNewDataframe(whole_df, SPX500_important, \"\")\n",
    "\n",
    "#FTSE 350 Index\n",
    "F3METL_original = pd.read_csv('../Datasets/Financial_dataset/F3METL.csv', index_col = False)\n",
    "F3METL          = wf.F3METL_Wrangler(F3METL_original)\n",
    "F3METL_important  = F3METL[['PX_LAST', 'PX_VOLUME']]\n",
    "whole_df        = addNewDataframe(whole_df, F3METL_important, \"F3METL\")\n",
    "\n",
    "original_data_GUKG10 = pd.read_csv('../../Notebooks/Datasets/Financial_dataset/GUKG10.csv')\n",
    "GUKG10  = wf.GUKG10_Wrangler(original_data_GUKG10)\n",
    "GUKG10_important = GUKG10[[('GUKG10 Index',   'PX_LAST')]]\n",
    "renameMultiIndex(GUKG10_important)\n",
    "whole_df         = addNewDataframe(whole_df, GUKG10_important, \"\")\n",
    "\n",
    "#BCOMIN\n",
    "original_BCOMIN = pd.read_csv('../../Notebooks/Datasets/Financial_dataset/BCOMIN.csv', index_col=False)\n",
    "BCOMIN = wf.BCOMIN_Wrangler(original_BCOMIN).set_index('Dates')\n",
    "BCOMIN_important = BCOMIN[['PX_LAST']]\n",
    "whole_df       = addNewDataframe(whole_df, BCOMIN_important, 'BCOMIN')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98dcdcb",
   "metadata": {},
   "source": [
    "#### Adding GDP growth and shipping costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f9a286b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vojno\\OneDrive - University of Bristol\\Applied Data Science\\Mining\\Notebooks\\Merging_Data\\WranglerFunctions.py:366: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  unique_values = df[column].unique()\n",
      "C:\\Users\\vojno\\OneDrive - University of Bristol\\Applied Data Science\\Mining\\Notebooks\\Merging_Data\\WranglerFunctions.py:366: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  unique_values = df[column].unique()\n",
      "C:\\Users\\vojno\\OneDrive - University of Bristol\\Applied Data Science\\Mining\\Notebooks\\Merging_Data\\WranglerFunctions.py:366: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  unique_values = df[column].unique()\n",
      "C:\\Users\\vojno\\OneDrive - University of Bristol\\Applied Data Science\\Mining\\Notebooks\\Merging_Data\\WranglerFunctions.py:366: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  unique_values = df[column].unique()\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'WranglerFunctions' has no attribute 'ShippingWrangler'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [50]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m whole_df \u001b[38;5;241m=\u001b[39m addNewDataframe(whole_df, us_GDP, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSA\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m shippingCosts \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../Datasets/Shipping_dataset/WCI_Freight_Rate_Composite.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index_col \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 18\u001b[0m shippingCosts \u001b[38;5;241m=\u001b[39m \u001b[43mwf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mShippingWrangler\u001b[49m(shippingCosts)\n\u001b[0;32m     19\u001b[0m whole_df \u001b[38;5;241m=\u001b[39m addNewDataframe(whole_df, shippingCosts, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShipping costs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'WranglerFunctions' has no attribute 'ShippingWrangler'"
     ]
    }
   ],
   "source": [
    "uk_GDP = pd.read_csv('../Datasets/Geography_dataset/uk_GDP.csv', index_col = False)\n",
    "uk_GDP = wf.gdpWrangler(uk_GDP)\n",
    "whole_df = addNewDataframe(whole_df, uk_GDP, \"UK\")\n",
    "\n",
    "china_GDP = pd.read_csv('../Datasets/Geography_dataset/chinaGDP.csv', index_col = False)\n",
    "china_GDP = wf.gdpWrangler(china_GDP)\n",
    "whole_df = addNewDataframe(whole_df, china_GDP, \"China\")\n",
    "\n",
    "japan_GDP = pd.read_csv('../Datasets/Geography_dataset/japan_GDP.csv', index_col = False)\n",
    "japan_GDP = wf.gdpWrangler(japan_GDP)\n",
    "whole_df = addNewDataframe(whole_df, japan_GDP, \"Japan\")\n",
    "\n",
    "us_GDP = pd.read_csv('../Datasets/Geography_dataset/us_GDP.csv', index_col = False)\n",
    "us_GDP = wf.gdpWrangler(us_GDP)\n",
    "whole_df = addNewDataframe(whole_df, us_GDP, \"USA\")\n",
    "\n",
    "shippingCosts = pd.read_csv('../Datasets/Shipping_dataset/WCI_Freight_Rate_Composite.csv', index_col = False)\n",
    "shippingCosts = wf.ShippingWrangler(shippingCosts)\n",
    "whole_df = addNewDataframe(whole_df, shippingCosts, \"Shipping costs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fec054",
   "metadata": {},
   "source": [
    "#### Correlation Heatmap to EBITDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a0844c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvertAllColumnsToFloat(df):\n",
    "    columnsToConvert = df.select_dtypes(exclude='float64')\n",
    "    columnNames = columnsToConvert.columns\n",
    "    for name in columnNames:\n",
    "        if name != \"Date\":\n",
    "            df[name] = df[name].astype('float64')\n",
    "    return df     \n",
    "    #for column_name in columnsToConvert\n",
    "\n",
    "def getEBITDAUnique(df, companyName): #Stolen from DataExploration\n",
    "    earnings_unique = df[companyName]['EBITDA'].drop_duplicates()\n",
    "    earnings_unique = pd.DataFrame(earnings_unique)\n",
    "    earnings_unique = earnings_unique.dropna()\n",
    "    earnings_unique = earnings_unique.reset_index()\n",
    "    return earnings_unique\n",
    "\n",
    "def singleCompaniesFeatureHeatmap(feature_df, ebitda_df, company):\n",
    "    feature_df = fillInNanValues(feature_df)  #IMPORTANT -> NEED TO DISCUSS WHETHER FFILL/BFILL IS OK!!!\n",
    "    feature_df = ConvertAllColumnsToFloat(feature_df)#Some columns have datatype object which .corr() ignores\n",
    "    companies_ebitda = getEBITDAUnique(ebitda_df, company)\n",
    "    RenameDateTimeColumnName(companies_ebitda)\n",
    "\n",
    "    companies_ebitda_vs_features = addNewDataframe(companies_ebitda, feature_df, \"\")\n",
    "\n",
    "    plt.subplots(figsize=(20,15))\n",
    "    ax = plt.axes()\n",
    "    ax.set_title(company)\n",
    "    heatmap = sb.heatmap(companies_ebitda_vs_features.corr(), cmap=\"YlGnBu\", annot=True)\n",
    "\n",
    "companies_financial_dataset = pd.read_csv('../../Notebooks/Datasets/Financial_dataset/F3METL_Comp.csv', index_col = False)\n",
    "companies_financial_dataset = wf.F3Metl_Comp_Wrangler(companies_financial_dataset)\n",
    "company_names = list(set(companies_financial_dataset.columns.get_level_values(0)))\n",
    "\n",
    "for name in company_names:\n",
    "    singleCompaniesFeatureHeatmap(whole_df, companies_financial_dataset, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65e4bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
